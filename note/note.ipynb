{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"A simple implementation of a greedy transition-based parser. Released under BSD license.\"\"\"\n",
    "from os import path\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "SHIFT = 0; RIGHT = 1; LEFT = 2;\n",
    "MOVES = (SHIFT, RIGHT, LEFT)\n",
    "START = ['-START-', '-START2-']\n",
    "END = ['-END-', '-END2-']\n",
    "\n",
    "\n",
    "class DefaultList(list):\n",
    "    \"\"\"A list that returns a default value if index out of bounds.\"\"\"\n",
    "    def __init__(self, default=None):\n",
    "        self.default = default\n",
    "        list.__init__(self)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            return list.__getitem__(self, index)\n",
    "        except IndexError:\n",
    "            return self.default\n",
    "\n",
    "\n",
    "class Parse(object):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.heads = [None] * (n-1)\n",
    "        self.labels = [None] * (n-1)\n",
    "        self.lefts = []\n",
    "        self.rights = []\n",
    "        for i in range(n+1):\n",
    "            self.lefts.append(DefaultList(0))\n",
    "            self.rights.append(DefaultList(0))\n",
    "\n",
    "    def add(self, head, child, label=None):\n",
    "        self.heads[child] = head\n",
    "        self.labels[child] = label\n",
    "        if child < head:\n",
    "            self.lefts[head].append(child)\n",
    "        else:\n",
    "            self.rights[head].append(child)\n",
    "\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, load=True):\n",
    "#        model_dir = os.path.dirname(__file__)\n",
    "        self.model = Perceptron(MOVES)\n",
    "        if load:\n",
    "            self.model.load(path.join(model_dir, 'parser.pickle'))\n",
    "        self.tagger = PerceptronTagger(load=load)\n",
    "        self.confusion_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def save(self):\n",
    "        self.model.save(path.join(os.path.dirname(__file__), 'parser.pickle'))\n",
    "        self.tagger.save()\n",
    "\n",
    "    def parse(self, words):\n",
    "        n = len(words)\n",
    "        i = 2; stack = [1]; parse = Parse(n)\n",
    "        tags = self.tagger.tag(words)\n",
    "        while stack or (i+1) < n:\n",
    "            features = extract_features(words, tags, i, n, stack, parse)\n",
    "            scores = self.model.score(features)\n",
    "            valid_moves = get_valid_moves(i, n, len(stack))\n",
    "            guess = max(valid_moves, key=lambda move: scores[move])\n",
    "            i = transition(guess, i, stack, parse)\n",
    "        return tags, parse.heads\n",
    "\n",
    "    def train_one(self, itn, words, gold_tags, gold_heads):\n",
    "        n = len(words)\n",
    "        i = 2; stack = [1]; parse = Parse(n)\n",
    "        tags = self.tagger.tag(words)\n",
    "        while stack or (i + 1) < n:\n",
    "            features = extract_features(words, tags, i, n, stack, parse)\n",
    "            scores = self.model.score(features)\n",
    "            valid_moves = get_valid_moves(i, n, len(stack))\n",
    "            gold_moves = get_gold_moves(i, n, stack, parse.heads, gold_heads)\n",
    "            guess = max(valid_moves, key=lambda move: scores[move])\n",
    "            try: assert gold_moves\n",
    "            except: gold_moves = valid_moves\n",
    "            best = max(gold_moves, key=lambda move: scores[move])\n",
    "            self.model.update(best, guess, features)\n",
    "            print(self.model.weights)\n",
    "            i = transition(guess, i, stack, parse)\n",
    "            self.confusion_matrix[best][guess] += 1\n",
    "        return len([i for i in range(n-1) if parse.heads[i] == gold_heads[i]])\n",
    "\n",
    "\n",
    "def transition(move, i, stack, parse):\n",
    "    if move == SHIFT:\n",
    "        stack.append(i)\n",
    "        return i + 1\n",
    "    elif move == RIGHT:\n",
    "        parse.add(stack[-2], stack.pop())\n",
    "        return i\n",
    "    elif move == LEFT:\n",
    "        parse.add(i, stack.pop())\n",
    "        return i\n",
    "    assert move in MOVES\n",
    "\n",
    "\n",
    "def get_valid_moves(i, n, stack_depth):\n",
    "    moves = []\n",
    "    if (i+1) < n:\n",
    "        moves.append(SHIFT)\n",
    "    if stack_depth >= 2:\n",
    "        moves.append(RIGHT)\n",
    "    if stack_depth >= 1:\n",
    "        moves.append(LEFT)\n",
    "    return moves\n",
    "\n",
    "\n",
    "def get_gold_moves(n0, n, stack, heads, gold):\n",
    "    def deps_between(target, others, gold):\n",
    "        for word in others:\n",
    "            if gold[word] == target or gold[target] == word:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    valid = get_valid_moves(n0, n, len(stack))\n",
    "    if not stack or (SHIFT in valid and gold[n0] == stack[-1]):\n",
    "        return [SHIFT]\n",
    "    if gold[stack[-1]] == n0:\n",
    "        return [LEFT]\n",
    "    costly = set([m for m in MOVES if m not in valid])\n",
    "    # If the word behind s0 is its gold head, Left is incorrect\n",
    "    if len(stack) >= 2 and gold[stack[-1]] == stack[-2]:\n",
    "        costly.add(LEFT)\n",
    "    # If there are any dependencies between n0 and the stack,\n",
    "    # pushing n0 will lose them.\n",
    "    if SHIFT not in costly and deps_between(n0, stack, gold):\n",
    "        costly.add(SHIFT)\n",
    "    # If there are any dependencies between s0 and the buffer, popping\n",
    "    # s0 will lose them.\n",
    "    if deps_between(stack[-1], range(n0+1, n-1), gold):\n",
    "        costly.add(LEFT)\n",
    "        costly.add(RIGHT)\n",
    "    return [m for m in MOVES if m not in costly]\n",
    "\n",
    "\n",
    "def extract_features(words, tags, n0, n, stack, parse):\n",
    "    def get_stack_context(depth, stack, data):\n",
    "        if depth >= 3:\n",
    "            return data[stack[-1]], data[stack[-2]], data[stack[-3]]\n",
    "        elif depth >= 2:\n",
    "            return data[stack[-1]], data[stack[-2]], ''\n",
    "        elif depth == 1:\n",
    "            return data[stack[-1]], '', ''\n",
    "        else:\n",
    "            return '', '', ''\n",
    "\n",
    "    def get_buffer_context(i, n, data):\n",
    "        if i + 1 >= n:\n",
    "            return data[i], '', ''\n",
    "        elif i + 2 >= n:\n",
    "            return data[i], data[i + 1], ''\n",
    "        else:\n",
    "            return data[i], data[i + 1], data[i + 2]\n",
    "\n",
    "    def get_parse_context(word, deps, data):\n",
    "        if word == -1:\n",
    "            return 0, '', ''\n",
    "        deps = deps[word]\n",
    "        valency = len(deps)\n",
    "        if not valency:\n",
    "            return 0, '', ''\n",
    "        elif valency == 1:\n",
    "            return 1, data[deps[-1]], ''\n",
    "        else:\n",
    "            return valency, data[deps[-1]], data[deps[-2]]\n",
    "\n",
    "    features = {}\n",
    "    # Set up the context pieces --- the word (W) and tag (T) of:\n",
    "    # S0-2: Top three words on the stack\n",
    "    # N0-2: First three words of the buffer\n",
    "    # n0b1, n0b2: Two leftmost children of the first word of the buffer\n",
    "    # s0b1, s0b2: Two leftmost children of the top word of the stack\n",
    "    # s0f1, s0f2: Two rightmost children of the top word of the stack\n",
    "\n",
    "    depth = len(stack)\n",
    "    s0 = stack[-1] if depth else -1\n",
    "\n",
    "    Ws0, Ws1, Ws2 = get_stack_context(depth, stack, words)\n",
    "    Ts0, Ts1, Ts2 = get_stack_context(depth, stack, tags)\n",
    "\n",
    "    Wn0, Wn1, Wn2 = get_buffer_context(n0, n, words)\n",
    "    Tn0, Tn1, Tn2 = get_buffer_context(n0, n, tags)\n",
    "\n",
    "    Vn0b, Wn0b1, Wn0b2 = get_parse_context(n0, parse.lefts, words)\n",
    "    Vn0b, Tn0b1, Tn0b2 = get_parse_context(n0, parse.lefts, tags)\n",
    "\n",
    "    Vn0f, Wn0f1, Wn0f2 = get_parse_context(n0, parse.rights, words)\n",
    "    _, Tn0f1, Tn0f2 = get_parse_context(n0, parse.rights, tags)\n",
    "\n",
    "    Vs0b, Ws0b1, Ws0b2 = get_parse_context(s0, parse.lefts, words)\n",
    "    _, Ts0b1, Ts0b2 = get_parse_context(s0, parse.lefts, tags)\n",
    "\n",
    "    Vs0f, Ws0f1, Ws0f2 = get_parse_context(s0, parse.rights, words)\n",
    "    _, Ts0f1, Ts0f2 = get_parse_context(s0, parse.rights, tags)\n",
    "\n",
    "    # Cap numeric features at 5?\n",
    "    # String-distance\n",
    "    Ds0n0 = min((n0 - s0, 5)) if s0 != 0 else 0\n",
    "\n",
    "    features['bias'] = 1\n",
    "    # Add word and tag unigrams\n",
    "    for w in (Wn0, Wn1, Wn2, Ws0, Ws1, Ws2, Wn0b1, Wn0b2, Ws0b1, Ws0b2, Ws0f1, Ws0f2):\n",
    "        if w:\n",
    "            features['w=%s' % w] = 1\n",
    "    for t in (Tn0, Tn1, Tn2, Ts0, Ts1, Ts2, Tn0b1, Tn0b2, Ts0b1, Ts0b2, Ts0f1, Ts0f2):\n",
    "        if t:\n",
    "            features['t=%s' % t] = 1\n",
    "\n",
    "    # Add word/tag pairs\n",
    "    for i, (w, t) in enumerate(((Wn0, Tn0), (Wn1, Tn1), (Wn2, Tn2), (Ws0, Ts0))):\n",
    "        if w or t:\n",
    "            features['%d w=%s, t=%s' % (i, w, t)] = 1\n",
    "\n",
    "    # Add some bigrams\n",
    "    features['s0w=%s,  n0w=%s' % (Ws0, Wn0)] = 1\n",
    "    features['wn0tn0-ws0 %s/%s %s' % (Wn0, Tn0, Ws0)] = 1\n",
    "    features['wn0tn0-ts0 %s/%s %s' % (Wn0, Tn0, Ts0)] = 1\n",
    "    features['ws0ts0-wn0 %s/%s %s' % (Ws0, Ts0, Wn0)] = 1\n",
    "    features['ws0-ts0 tn0 %s/%s %s' % (Ws0, Ts0, Tn0)] = 1\n",
    "    features['wt-wt %s/%s %s/%s' % (Ws0, Ts0, Wn0, Tn0)] = 1\n",
    "    features['tt s0=%s n0=%s' % (Ts0, Tn0)] = 1\n",
    "    features['tt n0=%s n1=%s' % (Tn0, Tn1)] = 1\n",
    "\n",
    "    # Add some tag trigrams\n",
    "    trigrams = ((Tn0, Tn1, Tn2), (Ts0, Tn0, Tn1), (Ts0, Ts1, Tn0),\n",
    "                (Ts0, Ts0f1, Tn0), (Ts0, Ts0f1, Tn0), (Ts0, Tn0, Tn0b1),\n",
    "                (Ts0, Ts0b1, Ts0b2), (Ts0, Ts0f1, Ts0f2), (Tn0, Tn0b1, Tn0b2),\n",
    "                (Ts0, Ts1, Ts1))\n",
    "    for i, (t1, t2, t3) in enumerate(trigrams):\n",
    "        if t1 or t2 or t3:\n",
    "            features['ttt-%d %s %s %s' % (i, t1, t2, t3)] = 1\n",
    "\n",
    "    # Add some valency and distance features\n",
    "    vw = ((Ws0, Vs0f), (Ws0, Vs0b), (Wn0, Vn0b))\n",
    "    vt = ((Ts0, Vs0f), (Ts0, Vs0b), (Tn0, Vn0b))\n",
    "    d = ((Ws0, Ds0n0), (Wn0, Ds0n0), (Ts0, Ds0n0), (Tn0, Ds0n0),\n",
    "         ('t' + Tn0+Ts0, Ds0n0), ('w' + Wn0+Ws0, Ds0n0))\n",
    "    for i, (w_t, v_d) in enumerate(vw + vt + d):\n",
    "        if w_t or v_d:\n",
    "            features['val/d-%d %s %d' % (i, w_t, v_d)] = 1\n",
    "    return features\n",
    "\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, classes=None):\n",
    "        # Each feature gets its own weight vector, so weights is a dict-of-arrays\n",
    "        self.classes = classes\n",
    "        self.weights = {}\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''Dot-product the features and current weights and return the best class.'''\n",
    "        scores = self.score(features)\n",
    "        # Do a secondary alphabetic sort, for stability\n",
    "        return max(self.classes, key=lambda clas: (scores[clas], clas))\n",
    "\n",
    "    def score(self, features):\n",
    "        all_weights = self.weights\n",
    "        scores = dict((clas, 0) for clas in self.classes)\n",
    "        for feat, value in features.items():\n",
    "            if value == 0:\n",
    "                continue\n",
    "            if feat not in all_weights:\n",
    "                continue\n",
    "            weights = all_weights[feat]\n",
    "            for clas, weight in weights.items():\n",
    "                scores[clas] += value * weight\n",
    "        return scores\n",
    "\n",
    "    def update(self, truth, guess, features):\n",
    "        def upd_feat(c, f, w, v):\n",
    "            param = (f, c)\n",
    "            self._totals[param] += (self.i - self._tstamps[param]) * w\n",
    "            self._tstamps[param] = self.i\n",
    "            self.weights[f][c] = w + v\n",
    "\n",
    "        self.i += 1\n",
    "        if truth == guess:\n",
    "            return None\n",
    "        for f in features:\n",
    "            weights = self.weights.setdefault(f, {})\n",
    "            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n",
    "            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
    "\n",
    "    def average_weights(self):\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = {}\n",
    "            for clas, weight in weights.items():\n",
    "                param = (feat, clas)\n",
    "                total = self._totals[param]\n",
    "                total += (self.i - self._tstamps[param]) * weight\n",
    "                averaged = round(total / float(self.i), 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[clas] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "\n",
    "    def save(self, path):\n",
    "        print \"Saving model to %s\" % path\n",
    "        pickle.dump(self.weights, open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        self.weights = pickle.load(open(path))\n",
    "\n",
    "\n",
    "class PerceptronTagger(object):\n",
    "    '''Greedy Averaged Perceptron tagger'''\n",
    "    # model_loc = os.path.join(os.path.dirname(__file__), 'tagger.pickle')\n",
    "    def __init__(self, classes=None, load=True):\n",
    "        self.tagdict = {}\n",
    "        if classes:\n",
    "            self.classes = classes\n",
    "        else:\n",
    "            self.classes = set()\n",
    "        self.model = Perceptron(self.classes)\n",
    "        if load:\n",
    "            self.load(PerceptronTagger.model_loc)\n",
    "\n",
    "    def tag(self, words, tokenize=True):\n",
    "        prev, prev2 = START\n",
    "        tags = DefaultList('')\n",
    "        context = START + [self._normalize(w) for w in words] + END\n",
    "        for i, word in enumerate(words):\n",
    "            tag = self.tagdict.get(word)\n",
    "            if not tag:\n",
    "                features = self._get_features(i, word, context, prev, prev2)\n",
    "                tag = self.model.predict(features)\n",
    "            tags.append(tag)\n",
    "            prev2 = prev; prev = tag\n",
    "        return tags\n",
    "\n",
    "    def start_training(self, sentences):\n",
    "        self._make_tagdict(sentences)\n",
    "        self.model = Perceptron(self.classes)\n",
    "\n",
    "    def train(self, sentences, save_loc=None, nr_iter=5):\n",
    "        '''Train a model from sentences, and save it at save_loc. nr_iter\n",
    "        controls the number of Perceptron training iterations.'''\n",
    "        self.start_training(sentences)\n",
    "        for iter_ in range(nr_iter):\n",
    "            for words, tags in sentences:\n",
    "                self.train_one(words, tags)\n",
    "            random.shuffle(sentences)\n",
    "        self.end_training(save_loc)\n",
    "\n",
    "    def save(self):\n",
    "        # Pickle as a binary file\n",
    "        pickle.dump((self.model.weights, self.tagdict, self.classes),\n",
    "                    open(PerceptronTagger.model_loc, 'wb'), -1)\n",
    "\n",
    "    def train_one(self, words, tags):\n",
    "        prev, prev2 = START\n",
    "        context = START + [self._normalize(w) for w in words] + END\n",
    "        for i, word in enumerate(words):\n",
    "            guess = self.tagdict.get(word)\n",
    "            if not guess:\n",
    "                feats = self._get_features(i, word, context, prev, prev2)\n",
    "                guess = self.model.predict(feats)\n",
    "                self.model.update(tags[i], guess, feats)\n",
    "            prev2 = prev; prev = guess\n",
    "\n",
    "    def load(self, loc):\n",
    "        w_td_c = pickle.load(open(loc, 'rb'))\n",
    "        self.model.weights, self.tagdict, self.classes = w_td_c\n",
    "        self.model.classes = self.classes\n",
    "\n",
    "    def _normalize(self, word):\n",
    "        if '-' in word and word[0] != '-':\n",
    "            return '!HYPHEN'\n",
    "        elif word.isdigit() and len(word) == 4:\n",
    "            return '!YEAR'\n",
    "        elif word[0].isdigit():\n",
    "            return '!DIGITS'\n",
    "        else:\n",
    "            return word.lower()\n",
    "\n",
    "    def _get_features(self, i, word, context, prev, prev2):\n",
    "        '''Map tokens into a feature representation, implemented as a\n",
    "        {hashable: float} dict. If the features change, a new model must be\n",
    "        trained.'''\n",
    "        def add(name, *args):\n",
    "            features[' '.join((name,) + tuple(args))] += 1\n",
    "\n",
    "        i += len(START)\n",
    "        features = defaultdict(int)\n",
    "        # It's useful to have a constant feature, which acts sort of like a prior\n",
    "        add('bias')\n",
    "        add('i suffix', word[-3:])\n",
    "        add('i pref1', word[0])\n",
    "        add('i-1 tag', prev)\n",
    "        add('i-2 tag', prev2)\n",
    "        add('i tag+i-2 tag', prev, prev2)\n",
    "        add('i word', context[i])\n",
    "        add('i-1 tag+i word', prev, context[i])\n",
    "        add('i-1 word', context[i-1])\n",
    "        add('i-1 suffix', context[i-1][-3:])\n",
    "        add('i-2 word', context[i-2])\n",
    "        add('i+1 word', context[i+1])\n",
    "        add('i+1 suffix', context[i+1][-3:])\n",
    "        add('i+2 word', context[i+2])\n",
    "        return features\n",
    "\n",
    "    def _make_tagdict(self, sentences):\n",
    "        '''Make a tag dictionary for single-tag words.'''\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "        for sent in sentences:\n",
    "            for word, tag in zip(sent[0], sent[1]):\n",
    "                counts[word][tag] += 1\n",
    "                self.classes.add(tag)\n",
    "        freq_thresh = 20\n",
    "        ambiguity_thresh = 0.97\n",
    "        for word, tag_freqs in counts.items():\n",
    "            tag, mode = max(tag_freqs.items(), key=lambda item: item[1])\n",
    "            n = sum(tag_freqs.values())\n",
    "            # Don't add rare words to the tag dictionary\n",
    "            # Only add quite unambiguous words\n",
    "            if n >= freq_thresh and (float(mode) / n) >= ambiguity_thresh:\n",
    "                self.tagdict[word] = tag\n",
    "\n",
    "def _pc(n, d):\n",
    "    return (float(n) / d) * 100\n",
    "\n",
    "\n",
    "def train(parser, sentences, nr_iter):\n",
    "    parser.tagger.start_training(sentences)\n",
    "    for itn in range(nr_iter):\n",
    "        corr = 0; total = 0\n",
    "        random.shuffle(sentences)\n",
    "        for words, gold_tags, gold_parse, gold_label in sentences:\n",
    "            corr += parser.train_one(itn, words, gold_tags, gold_parse)\n",
    "            if itn < 5:\n",
    "                parser.tagger.train_one(words, gold_tags)\n",
    "            total += len(words)\n",
    "        print itn, '%.3f' % (float(corr) / float(total))\n",
    "        if itn == 4:\n",
    "            parser.tagger.model.average_weights()\n",
    "    print 'Averaging weights'\n",
    "    parser.model.average_weights()\n",
    "\n",
    "def read_pos(loc):\n",
    "    for line in open(loc):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        words = DefaultList('')\n",
    "        tags = DefaultList('')\n",
    "        for token in line.split():\n",
    "            if not token:\n",
    "                continue\n",
    "            word, tag = token.rsplit('/', 1)\n",
    "            #words.append(normalize(word))\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "        pad_tokens(words); pad_tokens(tags)\n",
    "        yield words, tags\n",
    "\n",
    "\n",
    "def read_conll(loc):\n",
    "    for sent_str in open(loc).read().strip().split('\\n\\n'):\n",
    "        lines = [line.split() for line in sent_str.split('\\n')]\n",
    "        words = DefaultList(''); tags = DefaultList('')\n",
    "        heads = [None]; labels = [None]\n",
    "        for (_, word, _, pos, _, _, head, label, _, _) in lines:\n",
    "            words.append(intern(word))\n",
    "            #words.append(intern(normalize(word)))\n",
    "            tags.append(intern(pos))\n",
    "            heads.append(int(head) + 1 if head != '-1' else len(lines) + 1)\n",
    "            labels.append(label)\n",
    "        pad_tokens(words); pad_tokens(tags)\n",
    "        yield words, tags, heads, labels\n",
    "\n",
    "\n",
    "def pad_tokens(tokens):\n",
    "    tokens.insert(0, '<start>')\n",
    "    tokens.append('ROOT')\n",
    "\n",
    "\n",
    "def main(model_dir, train_loc, heldout_in, heldout_gold):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    #input_sents = list(read_pos(heldout_in))\n",
    "    parser = Parser(load=False)\n",
    "    sentences = list(read_conll(train_loc))\n",
    "    print(sentences[0])\n",
    "    exit()\n",
    "    train(parser, sentences, nr_iter=15)\n",
    "    parser.save()\n",
    "    c = 0\n",
    "    t = 0\n",
    "    gold_sents = list(read_conll(heldout_gold))\n",
    "    t1 = time.time()\n",
    "    for (words, tags), (_, _, gold_heads, gold_labels) in zip(input_sents, gold_sents):\n",
    "        _, heads = parser.parse(words)\n",
    "        for i, w in list(enumerate(words))[1:-1]:\n",
    "            if gold_labels[i] in ('P', 'punct'):\n",
    "                continue\n",
    "            if heads[i] == gold_heads[i]:\n",
    "                c += 1\n",
    "            t += 1\n",
    "    t2 = time.time()\n",
    "    print 'Parsing took %0.3f ms' % ((t2-t1)*1000.0)\n",
    "    print c, t, float(c)/t\n",
    "\n",
    "\n",
    "def train_pos(parser, sentences, nr_iter):\n",
    "    parser.tagger.start_training(sentences)\n",
    "    for itn in range(nr_iter):\n",
    "        print(itn)\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        for words, gold_tags, gold_parse, gold_label in sentences:\n",
    "            parser.tagger.train_one(words, gold_tags)\n",
    "\n",
    "    parser.tagger.model.average_weights()\n",
    "\n",
    "def train_dep(parser, sentences, nr_iter):\n",
    "    for itn in range(nr_iter):\n",
    "        corr = 0; total = 0\n",
    "        random.shuffle(sentences)\n",
    "        for words, gold_tags, gold_parse, gold_label in sentences:\n",
    "            corr += parser.train_one(itn, words, gold_tags, gold_parse)\n",
    "            total += len(words)\n",
    "        print itn, '%.3f' % (float(corr) / float(total))\n",
    "    print 'Averaging weights'\n",
    "    parser.model.average_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "model_dir = './model'; train_loc = '../data/train.conll'; heldout_int = '../data/test.conll'; heldout_gold = '../data/test_gold.conll'\n",
    "parser = Parser(load=False)\n",
    "sentences = list(read_conll(train_loc))\n",
    "\n",
    "train_pos(parser, sentences, nr_iter=5)\n",
    "\n",
    "# test_sents = list(read_conll(heldout_in))\n",
    "# tot = 0\n",
    "# acc = 0\n",
    "# for sent in test_sents:\n",
    "#     tags = parser.tagger.tag(sent[0])\n",
    "#     for guess, ans in zip(tags, sent[1]):\n",
    "#         if guess == ans:\n",
    "#             acc += 1\n",
    "#         tot += 1\n",
    "#\n",
    "# print(acc, tot, float(acc) / tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58846, 61516, 0.9565966577800897)\n"
     ]
    }
   ],
   "source": [
    "test_sents = list(read_conll(heldout_int))\n",
    "tot = 0\n",
    "acc = 0\n",
    "for sent in test_sents:\n",
    "    tags = parser.tagger.tag(sent[0])\n",
    "    for guess, ans in zip(tags, sent[1]):\n",
    "        if guess == ans:\n",
    "            acc += 1\n",
    "        tot += 1\n",
    "\n",
    "print(acc, tot, float(acc) / tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-bee3ee56c49b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-1d21a215669c>\u001b[0m in \u001b[0;36mtrain_dep\u001b[0;34m(parser, sentences, nr_iter)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_parse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mcorr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_parse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mitn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b83188dba86f>\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(self, itn, words, gold_tags, gold_heads)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgold_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_moves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_moves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b83188dba86f>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, truth, guess, features)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mupd_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mupd_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b83188dba86f>\u001b[0m in \u001b[0;36mupd_feat\u001b[0;34m(c, f, w, v)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupd_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_totals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tstamps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tstamps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dep(parser, sentences, nr_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, gold_tags, gold_parse, gold_label in sentences\n",
    "model = Perceptron(MOVES)\n",
    "\n",
    "n = len(words)\n",
    "i = 2; stack = [1]; parse = Parse(n)\n",
    "tags = parser.tagger.tag(words)\n",
    "features = extract_features(words, tags, i, n, stack, parse)\n",
    "scores = model.score(features)\n",
    "valid_moves = get_valid_moves(i, n, len(stack))\n",
    "gold_moves = get_gold_moves(i, n, stack, parse.heads, gold_parse)\n",
    "guess = max(valid_moves, key=lambda move: scores[move])\n",
    "best = max(gold_moves, key=lambda move: scores[move])\n",
    "model.update(best, guess, features)\n",
    "i = transition(guess, i, stack, parse)\n",
    "\n",
    "features = extract_features(words, tags, i, n, stack, parse)\n",
    "scores = model.score(features)\n",
    "valid_moves = get_valid_moves(i, n, len(stack))\n",
    "gold_moves = get_gold_moves(i, n, stack, parse.heads, gold_parse)\n",
    "guess = max(valid_moves, key=lambda move: scores[move])\n",
    "best = max(gold_moves, key=lambda move: scores[move])\n",
    "model.update(best, guess, features)\n",
    "i = transition(guess, i, stack, parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
